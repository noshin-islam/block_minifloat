import argparse
import time
import torch
import torch.nn.functional as F
from quant import *


torch.manual_seed(23)
data = (torch.randn(16)*1)
sign = torch.sign(data) 
data = (data**2)*sign


# result = block_B(data2, 8, max_func, 1)
# add some examples to test saturation limits

data = torch.cat([data, torch.tensor([7.666, 6.98, 7.01, 0.00879, 0.0142, 0.0158])])


torch.manual_seed(time.time())


# num = BlockMinifloat(exp=3, man=2, tile=-1)
num = BlockMinifloat(exp=4, man=3, tile=48, k_exp=1)
quant_func = quantizer(k=1,forward_number=num, forward_rounding="stochastic")

data_2 = torch.tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0.])

data2 = torch.tensor([[[[ 3.1222e-26, -6.1849e-26,  3.7319e-26, -3.5940e-26,
            1.1535e-26,  2.7104e-26],
          [ 3.2242e-26, -8.2415e-26,  1.1099e-25, -8.5107e-26,
           -1.7876e-25,  8.9582e-26],
          [ 6.6224e-26, -3.6070e-26, -6.2345e-26,  1.3148e-25,
            4.2742e-27, -6.6182e-26],
    
          [-1.3445e-25, -1.2027e-25,  9.8312e-29, -8.3284e-26,
           -3.6630e-26, -2.7607e-26],
          [ 4.1390e-26, -1.9599e-26,  4.3034e-26,  9.0267e-26,
           -1.1386e-25, -3.5463e-26],
          [ 7.6209e-26,  3.2669e-26,  1.7626e-26, -5.0374e-26,
           -4.7985e-26, -5.7851e-26]],

         [[-1.0155e-25, -4.4588e-26,  5.8665e-26,  1.0368e-26,
            1.0244e-25, -3.4247e-26],
          [-5.3204e-26, -1.4645e-26, -1.0781e-25,  1.9579e-25,
            3.6176e-26,  5.5195e-26],
          [-2.1273e-26, -9.1044e-26, -1.6957e-25, -2.6169e-25,
            2.3597e-26, -1.4212e-26],
    
          [ 6.1522e-26, -8.4360e-26,  6.8785e-26, -2.2347e-26,
            1.0612e-26,  1.4459e-26],
          [-5.1744e-26,  1.7974e-26, -9.4321e-26,  2.0998e-26,
            8.6753e-26,  1.8956e-26],
          [-8.2855e-26,  1.0218e-25,  3.7168e-26,  2.2230e-26,
            1.4327e-25,  1.2481e-25]],

         [[ 3.5215e-26, -9.1906e-26, -8.3157e-26, -6.6891e-27,
           -4.6038e-26,  3.4960e-26],
          [ 4.5379e-27,  6.7055e-26, -6.8101e-26,  1.4664e-26,
           -2.4065e-25, -9.7556e-26],
          [ 4.9326e-26, -6.9893e-26,  7.1467e-26, -5.3486e-26,
           -8.2462e-26, -5.5879e-26],
    
          [-3.5632e-26, -3.5612e-25,  8.3359e-26,  1.8774e-25,
            7.1156e-26, -1.1707e-26],
          [ 7.1670e-26, -9.1266e-26, -8.7943e-26, -1.0633e-25,
            1.2752e-26,  1.6677e-26],
          [ 1.1558e-26, -7.7193e-26, -2.8520e-26, -1.1522e-26,
            3.5432e-26, -2.1569e-26]],

    

         [[ 2.9525e-26, -4.1632e-26,  3.1783e-27, -1.0975e-26,
           -1.0498e-26, -1.1827e-26],
          [-2.6009e-26, -1.0684e-25, -1.4045e-26,  7.8281e-26,
           -4.6845e-26,  1.2877e-25],
          [ 1.4246e-26, -8.7252e-27, -6.0402e-27, -6.5277e-26,
            3.5692e-26,  1.6373e-26],
    
          [ 5.1393e-26,  2.7213e-26, -5.7635e-26,  2.1740e-26,
            3.9872e-26,  1.0660e-26],
          [ 6.3361e-26, -2.2714e-26,  3.4652e-26, -7.0610e-26,
           -3.0416e-26, -2.8808e-26],
          [ 1.6155e-26,  1.9059e-26,  1.7208e-26, -4.5433e-26,
           -4.0548e-26,  2.3086e-26]],

         [[-2.5946e-26,  1.5156e-26,  3.4603e-26,  7.5493e-27,
            2.8009e-26,  2.2507e-26],
          [-1.8444e-26,  3.7859e-26, -1.0958e-25, -2.5211e-26,
           -2.2381e-26, -1.9197e-26],
          [ 8.0474e-26, -5.5967e-26, -1.6402e-26, -5.4421e-26,
            3.1684e-26,  1.9704e-26],
    
          [-1.0660e-26,  6.7937e-26,  1.4489e-26,  8.6265e-26,
            4.0656e-26,  6.7723e-26],
          [-1.5991e-27, -5.4089e-26,  7.6893e-26, -2.6178e-26,
           -8.0810e-26, -9.2596e-27],
          [ 1.2798e-26,  6.7963e-26, -6.6639e-27,  1.4727e-26,
            9.9077e-27, -1.9428e-26]],

         [[-4.5018e-26,  2.1066e-27,  3.5831e-26,  1.0040e-26,
            2.9964e-26,  3.1187e-26],
          [ 2.6860e-27, -3.5087e-26,  4.2298e-26, -2.4269e-26,
           -4.9642e-26,  1.1811e-26],
          [ 4.2137e-26, -2.6458e-26,  2.3795e-26,  1.2160e-26,
           -3.9950e-26,  4.0865e-26],
    
          [ 4.8855e-26,  8.5239e-26,  4.5308e-26, -2.9308e-26,
           -3.0877e-26,  9.0131e-27],
          [ 3.2203e-26, -5.9365e-26,  2.4468e-26, -4.7299e-26,
           -1.3027e-26,  3.3419e-26],
          [-2.6231e-26,  3.7565e-26,  2.0371e-26,  9.5791e-26,
            1.3739e-26,  1.1880e-26]]]])

# print(data2)
# print(data2.shape)

# output = block_minifloat_quantize(data2, num, rounding="stochastic", tensor_type="x", k_exp=1, txt="hi")
output = quant_func(data2)
print("output ", output)





print("Input:", data2, "\n-----------------------------")
print("Quant:", output, "\n-----------------------------")
print("--------------------------------------")
print("Error:", data2-output, torch.sum((data2-output)**2)/(len(data2)))


#-1.2016e-04]]],   [[[ 1.8775e-06]]]])
